{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1da101af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing library\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "134ef821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing input file\n",
    "df=pd.read_excel('/home/sid/Downloads/Input.xlsx')[['URL_ID','URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb44f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "eec3a39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3        40  https://insights.blackcoffer.com/will-machine-...\n",
       "4        41  https://insights.blackcoffer.com/will-ai-repla...\n",
       "..      ...                                                ...\n",
       "109     146  https://insights.blackcoffer.com/blockchain-fo...\n",
       "110     147  https://insights.blackcoffer.com/the-future-of...\n",
       "111     148  https://insights.blackcoffer.com/big-data-anal...\n",
       "112     149  https://insights.blackcoffer.com/business-anal...\n",
       "113     150  https://insights.blackcoffer.com/challenges-an...\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7219eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('URL_ID',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a5913e",
   "metadata": {},
   "source": [
    "#### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4f66c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n",
      "/home/sid\n"
     ]
    }
   ],
   "source": [
    "url_id = 1\n",
    "for i in range(0, len(df)):\n",
    "    j = df.iloc[i].values\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}  # giving user access\n",
    "    page = requests.get(j[0], headers=headers)  # loading text from URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')  # parsing URL text\n",
    "    \n",
    "    # Extracting content\n",
    "    content = soup.find(attrs={'class': 'td-post-content'})\n",
    "    if content:\n",
    "        content_text = content.text.replace('\\xa0', \"  \").replace('\\n', \"  \")\n",
    "    else:\n",
    "        content_text = \"No Content Available\"  # Provide a default content message or handle as needed\n",
    "    \n",
    "    # Extracting title\n",
    "    title = soup.find(attrs={'class': 'entry-title'})\n",
    "    if title:\n",
    "        title_text = title.text.replace('\\n', \"  \").replace('/', \"\")\n",
    "    else:\n",
    "        title_text = \"No Title Available\"  # Provide a default title or handle as needed\n",
    "    \n",
    "    text = title_text + '.' + content_text  # merging title and content text\n",
    "    \n",
    "    b = str(url_id) + \".\" + 'txt'  # name of the text file\n",
    "    with open(b, 'w') as f:  # creating and writing to a text file\n",
    "        f.write(text)\n",
    "    import os\n",
    "    print(os.getcwd())\n",
    "    url_id += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aababf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id = 1\n",
    "for i in range(0, len(df)):\n",
    "    j = df.iloc[i].values\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}  # giving user access\n",
    "    page = requests.get(j[0], headers=headers)  # loading text from URL\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')  # parsing URL text\n",
    "    \n",
    "    # Extracting content, handling possible missing content gracefully\n",
    "    content = soup.find(attrs={'class': 'td-post-content'})\n",
    "    if content:\n",
    "        content = content.text.replace('\\xa0', \"  \").replace('\\n', \"  \")\n",
    "    else:\n",
    "        content = \"No Content Available\"  # Provide a default content message or handle as needed\n",
    "    \n",
    "    title = soup.findAll(attrs={'class': 'entry-title'})  # extracting title of website\n",
    "    \n",
    "    try:\n",
    "        title_text = title[url_id].text.replace('\\n', \"  \").replace('/', \"\")\n",
    "    except IndexError:\n",
    "        title_text = \"No Title Available\"  # Provide a default title or handle the situation as needed\n",
    "    \n",
    "    text = title_text + '.' + content  # merging title and content text\n",
    "    \n",
    "    b = str(url_id) + \".\" + 'txt'  # name of the text file\n",
    "    with open(b, 'w') as f:  # creating and writing to a text file\n",
    "        f.write(text)\n",
    "    \n",
    "    url_id += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68033f8",
   "metadata": {},
   "source": [
    "#### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "da4a7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing each extracted files\n",
    "text=pd.read_csv(\"/home/sid/114.txt\",header=None)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "57c53f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 39 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      " 1   1       1 non-null      object\n",
      " 2   2       1 non-null      object\n",
      " 3   3       1 non-null      object\n",
      " 4   4       1 non-null      object\n",
      " 5   5       1 non-null      object\n",
      " 6   6       1 non-null      object\n",
      " 7   7       1 non-null      object\n",
      " 8   8       1 non-null      object\n",
      " 9   9       1 non-null      object\n",
      " 10  10      1 non-null      object\n",
      " 11  11      1 non-null      object\n",
      " 12  12      1 non-null      object\n",
      " 13  13      1 non-null      object\n",
      " 14  14      1 non-null      object\n",
      " 15  15      1 non-null      object\n",
      " 16  16      1 non-null      object\n",
      " 17  17      1 non-null      object\n",
      " 18  18      1 non-null      object\n",
      " 19  19      1 non-null      object\n",
      " 20  20      1 non-null      object\n",
      " 21  21      1 non-null      object\n",
      " 22  22      1 non-null      object\n",
      " 23  23      1 non-null      object\n",
      " 24  24      1 non-null      object\n",
      " 25  25      1 non-null      object\n",
      " 26  26      1 non-null      object\n",
      " 27  27      1 non-null      object\n",
      " 28  28      1 non-null      object\n",
      " 29  29      1 non-null      object\n",
      " 30  30      1 non-null      object\n",
      " 31  31      1 non-null      object\n",
      " 32  32      1 non-null      object\n",
      " 33  33      1 non-null      object\n",
      " 34  34      1 non-null      object\n",
      " 35  35      1 non-null      object\n",
      " 36  36      1 non-null      object\n",
      " 37  37      1 non-null      object\n",
      " 38  38      1 non-null      object\n",
      "dtypes: object(39)\n",
      "memory usage: 440.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "#information of data frame\n",
    "text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "66b55bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing extra created column\n",
    "text.drop(1,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "77be1cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting type\n",
    "text=text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "76fb4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting text to sentence\n",
    "import re\n",
    "a=text[0].str.split('([\\.]\\s)',expand=False)#splitting text on '.'\n",
    "b=a.explode()#converting to rows\n",
    "b=pd.DataFrame(b)#creating data frame\n",
    "b.columns=['abc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4df5f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing . char from each rows\n",
    "def abcd(x):    \n",
    "    nopunc =[char for char in x if char != '.']\n",
    "    return ''.join(nopunc)\n",
    "b['abc']=b['abc'].apply(abcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e4c56cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing emty space with null values\n",
    "c=b.replace('',np.nan,regex=True)\n",
    "c=c.mask(c==\" \")\n",
    "c=c.dropna()\n",
    "c.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "20413991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automate the Data Management Process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Data  To begin with I shall first like to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big data is simply data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 abc\n",
       "0               Automate the Data Management Process\n",
       "1   Big Data  To begin with I shall first like to...\n",
       "2                            Big data is simply data"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f571da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nltk library and stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f245eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc=[punc for punc in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3d411059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5f8b13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each stop words file into a list with 'ISO-8859-1' encoding\n",
    "StopWords_Auditor = open(stop_words_directory + \"StopWords_Auditor.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_Currencies = open(stop_words_directory + \"StopWords_Currencies.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_DatesandNumbers = open(stop_words_directory + \"StopWords_DatesandNumbers.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_Generic = open(stop_words_directory + \"StopWords_Generic.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_GenericLong = open(stop_words_directory + \"StopWords_GenericLong.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_Geographic = open(stop_words_directory + \"StopWords_Geographic.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "StopWords_Names = open(stop_words_directory + \"StopWords_Names.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "\n",
    "# Now, StopWords_Auditor, StopWords_Currencies, and other lists contain the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "911fa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating func for removing stop words\n",
    "def text_process(text):\n",
    "    nopunc =[char for char in text if char not in punc or char not in [':',',','(',')','â€™','?']]\n",
    "    nopunc=''.join(nopunc)\n",
    "    txt=' '.join([word for word in nopunc.split() if word.lower() not in StopWords_Auditor])\n",
    "    txt1=' '.join([word for word in txt.split() if word.lower() not in StopWords_Currencies])\n",
    "    txt2=' '.join([word for word in txt1.split() if word.lower() not in StopWords_DatesandNumbers])\n",
    "    txt3=' '.join([word for word in txt2.split() if word.lower() not in StopWords_Generic])\n",
    "    txt4=' '.join([word for word in txt3.split() if word.lower() not in StopWords_GenericLong])\n",
    "    txt5=' '.join([word for word in txt4.split() if word.lower() not in StopWords_Geographic])\n",
    "    return ' '.join([word for word in txt5.split() if word.lower() not in StopWords_Names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cc4039e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying func for each row\n",
    "c['abc']=c['abc'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "051edc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automate Data Management Process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Data begin explain big data important lives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Big data simply data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               abc\n",
       "0                 Automate Data Management Process\n",
       "1  Big Data begin explain big data important lives\n",
       "2                             Big data simply data"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "961236e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read positive and negative words files into lists\n",
    "positive_list = open(master_dictionary_directory + \"positive-words.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "negative_list = open(master_dictionary_directory + \"negative-words.txt\", encoding='ISO-8859-1').read().splitlines()\n",
    "\n",
    "# Convert lists to pandas DataFrames\n",
    "positive = pd.DataFrame({'abc': positive_list})\n",
    "negative = pd.DataFrame({'abc': negative_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0540cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive.columns = ['abc']\n",
    "negative.columns = ['abc']\n",
    "positive['abc'] = positive['abc'].astype(str)\n",
    "negative['abc'] = negative['abc'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eb53427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive and negative dictionary without stopwords\n",
    "positive['abc']=positive['abc'].apply(text_process)\n",
    "negative['abc']=negative['abc'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "85be2e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive list\n",
    "length=positive.shape[0]\n",
    "post=[]\n",
    "for i in range(0,length):\n",
    "    nopunc =[char for char in positive.iloc[i] if char not in string.punctuation or char != '+']\n",
    "    nopunc=''.join(nopunc)\n",
    "\n",
    "    post.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ca9225be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative list\n",
    "length=negative.shape[0]\n",
    "neg=[]\n",
    "for i in range(0,length):\n",
    "    nopunc =[char for char in negative.iloc[i] if char not in string.punctuation or char != '+']\n",
    "    nopunc=''.join(nopunc)\n",
    "    neg.append(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c7516d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing tokenize library\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "50ffd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list=[]\n",
    "length=c.shape[0]\n",
    "for i in range(0,length):\n",
    "    txt=' '.join([word for word in c.iloc[i]])\n",
    "    txt_list.append(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d9d74356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization of text\n",
    "tokenize_text=[]\n",
    "for i in txt_list:\n",
    "  \n",
    "  tokenize_text+=(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "93f34981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Automate', 'Data', 'Management', 'Process', 'Big', 'Data', 'begin', 'explain', 'big', 'data', 'important', 'lives', 'Big', 'data', 'simply', 'data']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "30d4c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea1721",
   "metadata": {},
   "source": [
    "#### 1) POSITIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0ba9d626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postive score= 1\n"
     ]
    }
   ],
   "source": [
    "positive_score=0\n",
    "for i in tokenize_text:\n",
    "    if(i.lower() in post):\n",
    "        positive_score+=1\n",
    "print('postive score=', positive_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c278042d",
   "metadata": {},
   "source": [
    "#### 2) NEGATIVE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "59f93d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative score= 0\n"
     ]
    }
   ],
   "source": [
    "negative_score=0\n",
    "for i in tokenize_text:\n",
    "    if(i.lower() in neg):\n",
    "        negative_score+=1\n",
    "print('negative score=', negative_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d1bc5",
   "metadata": {},
   "source": [
    "#### 3) POLARITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3429d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity_score= 0.9999990000010001\n"
     ]
    }
   ],
   "source": [
    "#Polarity Score = (Positive Score â€“ Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "Polarity_Score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
    "print('polarity_score=', Polarity_Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c1677",
   "metadata": {},
   "source": [
    "#### 4) SUBJECTIVITY SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "62268d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjectivity_score 0.06249999609375024\n"
     ]
    }
   ],
   "source": [
    "#Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
    "subjectiivity_score=(positive_score-negative_score)/((len(tokenize_text))+ 0.000001)\n",
    "print('subjectivity_score',subjectiivity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb37bb2",
   "metadata": {},
   "source": [
    "#### 5) AVG SENTENCE LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0fcb08ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg sentence length= 33.0\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "    avg_length.append(len(c['abc'].iloc[i]))\n",
    "    avg_senetence_length=sum(avg_length)/len(avg_length)\n",
    "print('avg sentence length=', avg_senetence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08930484",
   "metadata": {},
   "source": [
    "#### 6) PERCENTAGE OF COMPLEX WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "07a6a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "    x=re.compile('[es|ed]$')\n",
    "    if x.match(i.lower()):\n",
    "        count+=0\n",
    "    else:\n",
    "        for j in i:\n",
    "            if(j.lower() in vowels ):\n",
    "                count+=1\n",
    "            if(count>2):\n",
    "                complex_Word_Count+=1\n",
    "                count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9ff6d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentag of complex words=  0.6875\n"
     ]
    }
   ],
   "source": [
    "Percentage_of_Complex_words=complex_Word_Count/len(tokenize_text)\n",
    "print('percentag of complex words= ',Percentage_of_Complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafec53",
   "metadata": {},
   "source": [
    "#### 7) FOG INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b8a79fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fog index=  13.475000000000001\n"
     ]
    }
   ],
   "source": [
    "#Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "Fog_Index = 0.4 * (avg_senetence_length + Percentage_of_Complex_words)\n",
    "print('fog index= ',Fog_Index )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33080760",
   "metadata": {},
   "source": [
    "#### 8) AVG NUMBER OF WORDS PER SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4bcb9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg no of words per sentence=  5.333333333333333\n"
     ]
    }
   ],
   "source": [
    "length=c.shape[0]\n",
    "avg_length=[]\n",
    "for i in range(0,length):\n",
    "    a=[word.split( ) for word in c.iloc[i]]\n",
    "    avg_length.append(len(a[0]))\n",
    "    a=0\n",
    "#avg\n",
    "avg_no_of_words_per_sentence=sum(avg_length)/length\n",
    "print(\"avg no of words per sentence= \",avg_no_of_words_per_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb10747",
   "metadata": {},
   "source": [
    "#### 9) COMPLEX WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "49f904c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex words count= 11\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "complex_Word_Count=0\n",
    "for i in tokenize_text:\n",
    "    x=re.compile('[es|ed]$')\n",
    "    if x.match(i.lower()):\n",
    "        count+=0\n",
    "    else:\n",
    "        for j in i:\n",
    "            if(j.lower() in vowels ):\n",
    "                count+=1\n",
    "            if(count>2):\n",
    "                complex_Word_Count+=1\n",
    "                count=0\n",
    "print('complex words count=',  complex_Word_Count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf094f9",
   "metadata": {},
   "source": [
    "#### 10) WORD COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "32d53ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count=  16\n"
     ]
    }
   ],
   "source": [
    "word_count=len(tokenize_text)\n",
    "print('word count= ', word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7521c4",
   "metadata": {},
   "source": [
    "#### 11) SYLLABLE PER WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "98dfb4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syllable_per_word=  35\n"
     ]
    }
   ],
   "source": [
    "vowels=['a','e','i','o','u']\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "    x=re.compile('[es|ed]$')\n",
    "    if x.match(i.lower()):\n",
    "        count+=0\n",
    "    else:\n",
    "        for j in i:\n",
    "            if(j.lower() in vowels ):\n",
    "                count+=1\n",
    "                syllable_count=count\n",
    "print('syllable_per_word= ',syllable_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0660514",
   "metadata": {},
   "source": [
    "#### 12) PERSONAL PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d1a8e507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "personal pronouns=  0\n"
     ]
    }
   ],
   "source": [
    "pronouns=['i','we','my','ours','us' ]\n",
    "import re\n",
    "count=0\n",
    "for i in tokenize_text:\n",
    "    if i.lower() in pronouns:\n",
    "        count+=1\n",
    "personal_pronouns=count\n",
    "print('personal pronouns= ',personal_pronouns )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbc23f",
   "metadata": {},
   "source": [
    "#### 13) AVG WORD LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "4516879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg word=  5.375\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in tokenize_text:\n",
    "    for j in i:\n",
    "        count+=1\n",
    "avg_word_length=count/len(tokenize_text)\n",
    "print('avg word= ', avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1059014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={'positive_score':positive_score,'negative_score':negative_score,'Polarity_Score':Polarity_Score,'subjectiivity_score':subjectiivity_score,'avg_senetence_length':avg_senetence_length,'Percentage_of_Complex_words':Percentage_of_Complex_words,'Fog_Index':Fog_Index,'avg_no_of_words_per_sentence':avg_no_of_words_per_sentence,'complex_Word_Count':complex_Word_Count,'word_count':word_count,'syllable_count':syllable_count,'personal_pronouns':personal_pronouns,'avg_word_length':avg_word_length}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d2baaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_129457/3429486942.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output = output.append(data, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>Polarity_Score</th>\n",
       "      <th>subjectiivity_score</th>\n",
       "      <th>avg_senetence_length</th>\n",
       "      <th>Percentage_of_Complex_words</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>avg_no_of_words_per_sentence</th>\n",
       "      <th>complex_Word_Count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>syllable_count</th>\n",
       "      <th>personal_pronouns</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>13.475</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   positive_score  negative_score  Polarity_Score  subjectiivity_score  \\\n",
       "0             1.0             0.0        0.999999               0.0625   \n",
       "\n",
       "   avg_senetence_length  Percentage_of_Complex_words  Fog_Index  \\\n",
       "0                  33.0                       0.6875     13.475   \n",
       "\n",
       "   avg_no_of_words_per_sentence  complex_Word_Count  word_count  \\\n",
       "0                      5.333333                11.0        16.0   \n",
       "\n",
       "   syllable_count  personal_pronouns  avg_word_length  \n",
       "0            35.0                0.0            5.375  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define an empty DataFrame\n",
    "output = pd.DataFrame()\n",
    "\n",
    "# Append data to the output DataFrame\n",
    "output = output.append(data, ignore_index=True)\n",
    "\n",
    "# Now you can access and use the output DataFrame\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a731890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
